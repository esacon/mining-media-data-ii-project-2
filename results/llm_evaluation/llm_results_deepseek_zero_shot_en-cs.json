{
  "model_type": "deepseek",
  "prompt_type": "zero_shot",
  "language_pair": "en-cs",
  "sample_size": 50,
  "metrics": {
    "accuracy": 0.86,
    "mcc": 0.21177936240075526,
    "precision": 0.5,
    "recall": 0.14285714285714285,
    "f1": 0.2222222222222222
  },
  "predictions": [
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "ERR",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "ERR",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT"
  ],
  "true_labels": [
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "ERR",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "ERR",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "ERR",
    "NOT",
    "NOT",
    "ERR",
    "ERR",
    "NOT",
    "ERR",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "ERR",
    "NOT"
  ],
  "responses": [
    "?\n\nThe target language is Czech.\n\nPlease classify it and explain your reasoning step by step.\nFirst, let's understand what a critical error is based on the given categories:\n\n1. **Toxicity**: Hate speech, violence, profanity",
    "?\n\nThe source text contains a reference to \"Marc\" and mentions \"commercial recordings of The Mikado\". \n\nIn the target language, it translates to \"Koneckonc\u016f je to jedna z - mo\u017en\u00e1 i",
    "?\n\nFirst, let's break down the source text into segments that might be prone to critical errors:\n\n1. \"Mel, they were already cited.\" - This segment involves a named entity (\"Mel\") and potentially toxicity if misinterpreted, but",
    "?\n\nThe source text is in English and target in Czech. We have a list of potential critical errors. Let's see if there's any issue here.\n\nFirst, check toxicity. Nothing seems hateful, violent, or with profanity changes.",
    "?\n\nThe target language is Czech.\nBased on the provided classification criteria and ignoring minor grammar or style issues, I need to evaluate if there's a critical error in the given machine translation. The source text is \"Knock of the vand",
    "?\n\nThe target language should be Czech. The source text contains a statement about the relationship between two languages and an exclamation of repetition.\nIn the target translation, \"Maltese\" is translated to \"Malt\u0161tina\", which seems",
    "?\n\nThe target language is Czech.\nThis source text contains a potentially inflammatory statement about political parties and their alignment with negative traits (\"evil\"). The target translation in Czech mirrors this sentiment exactly, using \"z l\u00ed\" to convey \"evil\". This",
    "?\n\nThe source language is English and target language is Czech. The system should be able to detect if there's a critical error regardless of the languages involved.\n\nIn the source sentence, \"subscribe\" might mean following on social media or subscribing to",
    "?\n\nThe source text has two categories: \"American agnostics\" and \"People with Bipolar Disorder\". The target translates it to \"Americk\u00e1 agnostika\" and \"Lid\u00e9 s bipol\u00e1rn\u00ed por",
    "Okay, let's break down this example:\n\nFirst, we have to check if there is any toxicity in the target text. The target says \"V sou\u010dasn\u00e9 dob\u011b 2. odstavec nezmi\u0148uje"
  ]
}