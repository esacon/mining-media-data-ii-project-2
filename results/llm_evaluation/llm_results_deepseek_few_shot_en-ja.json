{
  "model_type": "deepseek",
  "prompt_type": "few_shot",
  "language_pair": "en-ja",
  "sample_size": 50,
  "metrics": {
    "accuracy": 0.86,
    "mcc": 0.0,
    "precision": 0.0,
    "recall": 0.0,
    "f1": 0.0
  },
  "predictions": [
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT"
  ],
  "true_labels": [
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "ERR",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "ERR",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "ERR",
    "NOT",
    "ERR",
    "ERR",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "ERR",
    "ERR",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT",
    "NOT"
  ],
  "responses": [
    "?\n\nBased on the provided classification examples, we need to determine if there's any critical error in the given translation pair.\n\nFirst, let's break down the source sentence:\n\n\"I'm probably getting inked later this month, and I intend to",
    "?\n\nBased on the provided examples and guidelines, what should be the label for this translation?\nThe target language is Japanese.\n\nFirst, let's check for toxicity: There's no hate speech, violence, or profanity change here. It seems",
    "?\n\nBased on the provided classification examples and guidelines, determine if there is any critical error in the target language. If yes, label it as \"ERR\", otherwise \"NOT\". \n\nThe source text contains questions about whether disabling traction control during engine",
    "?\n\nBased on the provided classification examples, we need to determine if there's any critical error in the given source-target pair.\n\nFirst, let's break down the target sentence:\n\n\"\u79c1 \u304c \u3057 \u305f \u3044 \u3068\u304d",
    "?\n\nBased on the provided classification examples and guidelines, determine if there is any critical error. If yes, label it as \"ERR\", otherwise \"NOT\". Remember that only critical errors should be flagged; ignore minor grammar, style, or ph",
    "?\n\nBased on the provided examples and guidelines, determine if there is a critical error in the target language with respect to the source text. If yes, label it as \"ERR\", otherwise \"NOT\". Remember that only specific types of errors should",
    "?\n\nBased on the provided classification examples and guidelines, determine if there is a critical error in the target language. If yes, label it as \"ERR\", otherwise \"NOT\". Remember that critical errors must be significant enough to change meaning or introduce",
    "?\n\nBased on the provided classification guidelines, we need to determine if there is any critical error in the given machine translation. \n\nFirst, let's break down the source sentence: \"This site is rubbish and is possibly slanderous. I hope",
    "?\n\nBased on the provided examples and guidelines, determine if there is a critical error in the target language output.\nFirst, let's break down the source sentence:\n\n\"Who you are to tell anything about some nationalistic nonsense? smells very russian",
    "?\n\nBased on the provided classification examples and guidelines, determine if there is any critical error in the target language output relative to the source input. If yes, label it as \"ERR\", otherwise \"NOT\". \n\nFirst, analyze the content step"
  ]
}