# Project Structure

This document provides a detailed overview of the project architecture, components, and data flow.

## ğŸ—ï¸ Directory Structure

```
project-2/
â”œâ”€â”€ config.yaml                     # Main configuration file
â”œâ”€â”€ Makefile                        # Build and workflow automation
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ pyproject.toml                  # Code formatting and tool configuration
â”œâ”€â”€ setup.py                       # Package installation setup
â”œâ”€â”€ Pipfile / Pipfile.lock         # Pipenv dependency management
â”‚
â”œâ”€â”€ data/                           # Data directory
â”‚   â”œâ”€â”€ catastrophic_errors/        # Training/dev/test datasets
â”‚   â”‚   â”œâ”€â”€ encs_majority_train.tsv
â”‚   â”‚   â”œâ”€â”€ encs_majority_dev.tsv
â”‚   â”‚   â”œâ”€â”€ encs_majority_test_blind.tsv
â”‚   â”‚   â”œâ”€â”€ ende_majority_*.tsv
â”‚   â”‚   â”œâ”€â”€ enja_majority_*.tsv
â”‚   â”‚   â””â”€â”€ enzh_majority_*.tsv
â”‚   â”œâ”€â”€ catastrophic_errors_goldlabels/  # Test gold labels
â”‚   â”‚   â”œâ”€â”€ encs_majority_test_goldlabels.tar.gz
â”‚   â”‚   â”œâ”€â”€ ende_majority_test_goldlabels.tar.gz
â”‚   â”‚   â”œâ”€â”€ enja_majority_test_goldlabels.tar.gz
â”‚   â”‚   â””â”€â”€ enzh_majority_test_goldlabels.tar.gz
â”‚   â””â”€â”€ evaluation/                 # Official evaluation scripts
â”‚       â”œâ”€â”€ README.md
â”‚       â””â”€â”€ sent_evaluate_CED.py
â”‚
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ runner.py                  # Main pipeline orchestrator
â”‚   â”œâ”€â”€ data/                      # Data handling components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_loader.py         # Data loading and preprocessing
â”‚   â”‚   â””â”€â”€ dataset.py             # PyTorch dataset implementation
â”‚   â”œâ”€â”€ models/                    # Model components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ distilbert_classifier.py  # DistilBERT-based classifier
â”‚   â”‚   â”œâ”€â”€ trainer.py             # Training logic and loops
â”‚   â”‚   â””â”€â”€ evaluation_formatter.py   # WMT evaluation format output
â”‚   â”œâ”€â”€ types/                     # Type definitions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ types.py               # Custom types and data structures
â”‚   â””â”€â”€ utils/                     # Utility modules
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py              # Configuration management
â”‚       â”œâ”€â”€ logging_utils.py       # Logging utilities
â”‚       â””â”€â”€ language_utils.py      # Language pair utilities
â”‚
â”œâ”€â”€ scripts/                       # Command-line scripts
â”‚   â”œâ”€â”€ run_pipeline.py           # Main CLI entry point
â”‚   â”œâ”€â”€ setup_dev.sh              # Development environment setup
â”‚   â””â”€â”€ test_all_languages.sh     # Multi-language training script
â”‚
â”œâ”€â”€ logs/                         # Training and execution logs
â”‚   â””â”€â”€ (generated during runtime)
â”‚
â”œâ”€â”€ results/                      # Output directory
â”‚   â”œâ”€â”€ checkpoints/              # Model checkpoints
â”‚   â”‚   â”œâ”€â”€ best_model_*.pt       # Best model during training
â”‚   â”‚   â”œâ”€â”€ final_model_*.pt      # Final trained model
â”‚   â”‚   â””â”€â”€ training_history_*.json  # Training metrics
â”‚   â”œâ”€â”€ prediction_results.json   # Prediction outputs
â”‚   â”œâ”€â”€ evaluation_*.tsv          # WMT evaluation format
â”‚   â””â”€â”€ evaluation_metadata_*.json # Model metadata
â”‚
â””â”€â”€ references/                   # Documentation and references
    â””â”€â”€ MMD_II_SoSe25_Assignment2.pdf
```

## ğŸ§© Component Architecture

### Core Components

#### 1. **Main Pipeline (`src/runner.py`)**
- Central orchestrator for all operations
- Handles training, evaluation, prediction workflows
- Manages configuration and logging
- Coordinates between data, model, and utility components

#### 2. **Data Layer (`src/data/`)**
- **`data_loader.py`**: Loads and preprocesses TSV data files
- **`dataset.py`**: PyTorch Dataset implementation for batch processing
- Handles tokenization, encoding, and data augmentation

#### 3. **Model Layer (`src/models/`)**
- **`distilbert_classifier.py`**: DistilBERT-based binary classifier
- **`trainer.py`**: Training loops, validation, and model checkpointing
- **`evaluation_formatter.py`**: Converts predictions to WMT evaluation format

#### 4. **Utilities (`src/utils/`)**
- **`config.py`**: YAML configuration loading and validation
- **`logging_utils.py`**: Structured logging and progress tracking
- **`language_utils.py`**: Language pair processing utilities

#### 5. **Types (`src/types/`)**
- **`types.py`**: Custom type definitions and data structures
- Ensures type safety across the codebase

### CLI Interface (`scripts/run_pipeline.py`)
Command-line interface that provides:
- Argument parsing and validation
- Command routing (train/evaluate/predict/analyze)
- Debug mode support
- Integration with the main pipeline

## ğŸ”„ Data Flow

### Training Flow
```
TSV Data Files â†’ DataLoader â†’ PyTorch Dataset â†’ DistilBERT â†’ Training Loop â†’ Model Checkpoints
```

1. **Data Loading**: TSV files loaded and validated
2. **Preprocessing**: Text tokenization and encoding
3. **Model Training**: DistilBERT fine-tuning with binary classification head
4. **Checkpointing**: Best models saved based on validation metrics
5. **Logging**: Training metrics and progress tracked

### Evaluation Flow
```
Trained Model â†’ Test Data â†’ Predictions â†’ Metrics Calculation â†’ Results Output
```

1. **Model Loading**: Restore trained model from checkpoint
2. **Data Processing**: Prepare test data for inference
3. **Prediction**: Generate binary classifications
4. **Evaluation**: Calculate MCC, accuracy, F1, precision, recall
5. **Output**: JSON results and optional WMT evaluation format

### Prediction Flow
```
Model + Test Data â†’ Inference â†’ Probability Scores â†’ WMT Format â†’ Evaluation Files
```

1. **Inference**: Generate predictions on blind test data
2. **Post-processing**: Convert probabilities to binary labels
3. **Format Conversion**: Create WMT evaluation format files
4. **Metadata**: Generate model statistics and configuration info

## ğŸ“Š Model Architecture

### DistilBERT Classifier
- **Base Model**: `distilbert-base-multilingual-cased`
- **Task**: Binary sequence classification
- **Input Format**: `[CLS] source_text [SEP] target_text [SEP]`
- **Output**: Binary classification (ERR/NOT)
- **Max Sequence Length**: 512 tokens

### Training Configuration
```yaml
model:
  name: "distilbert-base-multilingual-cased"
  num_labels: 2
  max_seq_length: 512

training:
  batch_size: 16
  learning_rate: 2.0e-5
  num_epochs: 3
  weight_decay: 0.01
  warmup_steps: 500

data:
  train_ratio: 0.8
  val_ratio: 0.1
  random_seed: 42
```

## ğŸ”§ Configuration Management

### Configuration Hierarchy
1. **Default Configuration**: Hardcoded defaults in `src/utils/config.py`
2. **YAML File**: Main configuration in `config.yaml`
3. **Command Line**: Override via CLI arguments
4. **Environment Variables**: Runtime overrides

### Debug Mode
- **Purpose**: Fast testing with small datasets
- **Activation**: `--debug-test` flag
- **Effects**: 
  - Reduced dataset size (100 samples)
  - Fewer training epochs
  - Simplified logging

## ğŸ“ Output Files

### Model Outputs
- **Checkpoints**: `results/checkpoints/best_model_epoch_X.pt`
- **Final Model**: `results/checkpoints/final_model_TIMESTAMP_LANG.pt`
- **Training History**: `results/checkpoints/training_history_TIMESTAMP_LANG.json`

### Prediction Outputs
- **JSON Results**: `results/prediction_results.json`
- **WMT Format**: `results/evaluation_LANG_METHOD.tsv`
- **Metadata**: `results/evaluation_metadata_LANG_METHOD.json`

## ğŸš€ Build System (Makefile)

### Target Categories
1. **Core Operations**: `train`, `evaluate`, `predict`, `analyze`
2. **Debug Mode**: `debug-train`, `debug-evaluate`, `debug-predict`, `debug-experiment`
3. **Development**: `install`, `format`, `lint`, `clean`, `status`

### Variable System
- **LANG**: Language pair selection (en-de, en-ja, en-zh, en-cs)
- **MODEL**: Custom model path override
- **SAMPLE_SIZE**: Dataset size limiting for testing
- **NO_EVAL_FORMAT**: Skip WMT evaluation format generation
